{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gu68vdLBlYX"
      },
      "source": [
        "# Week 11 Assignment\n",
        "\n",
        "\n",
        "Please do the programming exercise and verify that your code works using the tests, then think about your final project and fill out the questions in the second part.\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKfOJnHABlYY"
      },
      "source": [
        "### 47.1: Filtering and summarizing data\n",
        "\n",
        "For this work, you'll find a data file in `https://hds5210-data.s3.amazonaws.com/complications_all.csv`.\n",
        "\n",
        "Read in the data file and create a variable called `mo_hospitals` that contains a data frame from the `complications_all.csv` file, filtered down to only contain those hospitals from the state of Missouri (MO).\n",
        "\n",
        "Then aggregate that data by hospital into a variable named `mo_summary`.  There are some key fields that we want to summarize:\n",
        "* We want to know the earliest date that each hospital was participating in any program\n",
        "* We want to know the latest date that each hospital stopped participating in any program\n",
        "* We want to know the total number of patients in the denominators of these programs\n",
        "\n",
        "Some things to note:\n",
        "* You will need to convert the `Start Date` and `End Date` to actual datetime fields\n",
        "* You will need to clean up and convert the `Denominator` field to just be numeric - the rule that you should use it to simply remove any records where the `Denominator` is `'Not Available'`\n",
        "\n",
        "\n",
        "The final result of this step should be a new data frame called `mo_summary` that contains one row for each hospital and contains the min start date, max end date, and total denominator.  Use the names `start_date`, `end_date`, and `number` for those columns in `mo_summary`.\n",
        "\n",
        "\n",
        "You do not need to create your code in the form of a function, just make sure your variable names match what I've described above so the tests work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YMLIcVPFBlYZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# This is just to show you the name to use for the variable you need to create for this step to pass.\n",
        "all_hospitals = pd.read_csv('https://hds5210-data.s3.amazonaws.com/complications_all.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GkSKXulnBlYa",
        "outputId": "27d6244d-b8a6-4050-801d-7cfea08893b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    start_date   end_date  number\n",
            "Facility Name                                                    \n",
            "BARNES JEWISH HOSPITAL              2015-04-01 2018-06-30  131313\n",
            "BARNES-JEWISH ST PETERS HOSPITAL    2015-04-01 2018-06-30   15668\n",
            "BARNES-JEWISH WEST COUNTY HOSPITAL  2015-04-01 2018-06-30    9622\n",
            "BATES COUNTY MEMORIAL HOSPITAL      2015-07-01 2018-06-30    3117\n",
            "BELTON REGIONAL MEDICAL CENTER      2015-04-01 2018-06-30    9270\n",
            "...                                        ...        ...     ...\n",
            "TRUMAN MEDICAL CENTER LAKEWOOD      2015-04-01 2018-06-30    4297\n",
            "UNIVERSITY OF MISSOURI HEALTH CARE  2015-04-01 2018-06-30   56493\n",
            "WASHINGTON COUNTY MEMORIAL HOSPITAL 2015-07-01 2018-06-30     220\n",
            "WESTERN MISSOURI MEDICAL CENTER     2015-04-01 2018-06-30    7254\n",
            "WRIGHT MEMORIAL HOSPITAL            2015-07-01 2018-06-30     198\n",
            "\n",
            "[108 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-dfe83498f233>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mo_hospitals['Denominator'] = pd.to_numeric(mo_hospitals['Denominator'], errors='coerce')\n"
          ]
        }
      ],
      "source": [
        "url = \"https://hds5210-data.s3.amazonaws.com/complications_all.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "mo_hospitals = data[data['State'] == 'MO']\n",
        "mo_hospitals['Denominator'] = pd.to_numeric(mo_hospitals['Denominator'], errors='coerce')\n",
        "mo_hospitals = mo_hospitals.dropna(subset=['Denominator'])\n",
        "mo_hospitals['Denominator'] = mo_hospitals['Denominator'].astype('int')\n",
        "mo_hospitals['Start Date'] = pd.to_datetime(mo_hospitals['Start Date'], errors='coerce')\n",
        "mo_hospitals['End Date'] = pd.to_datetime(mo_hospitals['End Date'], errors='coerce')\n",
        "mo_summary = mo_hospitals.groupby('Facility Name').agg(\n",
        "    start_date=('Start Date', 'min'),\n",
        "    end_date=('End Date', 'max'),\n",
        "    number=('Denominator', 'sum')\n",
        ").reset_index()\n",
        "mo_summary.set_index('Facility Name', inplace=True)\n",
        "print(mo_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zIUDxVhfBlYb"
      },
      "outputs": [],
      "source": [
        "assert(mo_summary['number'].sum() == 1766908)\n",
        "assert(mo_summary['start_date'].min() == pd.Timestamp(2015,4,1))\n",
        "assert(mo_summary['end_date'].max() == pd.Timestamp(2018,6,30))\n",
        "assert(mo_summary.shape == (108,3))\n",
        "assert(mo_summary.loc['BARNES JEWISH HOSPITAL'].number == 131313)\n",
        "assert(mo_summary.loc['BOONE HOSPITAL CENTER'].number == 63099)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edbHbXPlBlYb"
      },
      "source": [
        "---\n",
        "\n",
        "### 47.2 Planning your final project\n",
        "\n",
        "You should be thinking about the things we've been learning and how you can apply them to your final project.  Use the rubric to help guid your thinking and then answer the questions below.  This is meant as a guide to help you think through what you will do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPSFac5RBlYc"
      },
      "source": [
        "#### A) Data Access\n",
        "\n",
        "Your project should include data from at least three distinct types of sources.  For example: AWS S3, Relational Databases, Internet, Web Services, local files.  List what data sources you're planning to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXVK_WVoBlYc"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "\n",
        "My project will use two sources. One of them is the dataset that is hosted on AWS S3, containing very large raw data files in formats like CSV, JSON, and Parquet. The output from these files would allow me to become familiar with hands-on processing and analysis of large amounts of data to study the challenges and techniques related to handling big data efficiently.\n",
        "\n",
        "More importantly, I will incorporate data from relational databases such as MySQL and PostgreSQL. This will present me with the chance to work with structured data, constructing extensive queries and carrying out analyses, especially related to business applications or transactional data. The idea here is to get a more complete picture by combining both sources in terms of data integration, wrangling, and analysis on various platforms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlFDDBjUBlYc"
      },
      "source": [
        "#### B. Data Formats\n",
        "\n",
        "Your project should include data that comes in different file formats.  For example: HL7, EDI, HTML, CSV, Excel, JSON, XML.  List what data formats you're planning to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTuDY1hlBlYc"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "\n",
        "So, for my final project, I plan to work with two kinds of data: **CSV** and **JSON**. In the CSV format, this will enable me to work with tabular data easily using libraries such as pandas for manipulation. JSON will allow handling semi-structured data; Web APIs represent one of the most popular ways to get them, and this would be useful in integrating several sources of data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCaIBMOOBlYd"
      },
      "source": [
        "#### C. Objective\n",
        "\n",
        "What purpose would your project serve in a real work setting?  Take a couple of paragraphs to write down why this is an interesting product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBaHH_rVBlYd"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "This will no doubt be a very useful project in the real world, where there is a need for integration and analysis of data coming from different sources in an organizational setup. The integration of data from AWS S3 with relational databases will ensure that business gains in-depth insight into their operations, customer behaviors, and correspondingly, trends. Access to big data in cloud storage, such as AWS S3, for processing and structured data from relational databases like MySQL and PostgreSQL, will surely further equip the decision-makers with meaningful insights that drive strategic decisions in business across different dimensions of operational efficiency analysis, sales performance analysis, and customer demographic analysis.\n",
        "\n",
        "This is remarkably important, as it allows organizations to come up with better predictive models and reports that will make use of both the structured and unstructured data. Thirdly, the automation also caters for speed and scalability of data analysis. Real-time or current data from web services, integrated with historic data from databases and cloud storage, can give actionable insights for operational adjustments and long-term planning. Such projects find very relevant applications in industries such as healthcare, finance, and e-commerce, where insight-driven decisions are extremely important to competitiveness and performance optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtxyMx-jBlYd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Submit your work via GitHub as normal\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}